import os
import time
import logging
import subprocess
import numpy as np
import pandas as pd
from flask import Flask, request, jsonify
from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST
from backend import create_app
from backend.utils.models_loader import load_models
from waitress import serve
from functools import wraps
from flasgger import swag_from
from backend.utils.security import api_key_required

# âœ… Configuration du logger
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s", force=True)

# âœ… Initialisation de l'application Flask via create_app()
app = create_app()

logging.info("âœ… Flask est initialisÃ© avec succÃ¨s !")

# âœ… VÃ©rification de l'API Key (force string pour Ã©viter NoneType)
API_KEY = str(os.getenv("API_KEY", "")).strip()
if not API_KEY:
    logging.warning("âš ï¸ La variable d'environnement API_KEY est vide ou non dÃ©finie.")
else:
    logging.info(f"ğŸ”‘ API_KEY chargÃ©e : {API_KEY[:5]}*** (sÃ©curisÃ©e)")

# âœ… Endpoint pour dÃ©boguer la clÃ© API
@app.route('/debug_api_key', methods=['GET'])
def debug_api_key():
    return jsonify({
        "api_key_stockee": API_KEY,
        "api_key_reÃ§ue": request.headers.get("x-api-key")
    }), 200

# âœ… DÃ©finition des mÃ©triques Prometheus
REQUEST_COUNT = Counter("api_requests_total", "Nombre total de requÃªtes", ["method", "endpoint", "http_status"])
REQUEST_LATENCY = Histogram("api_request_latency_seconds", "Temps de rÃ©ponse des requÃªtes", ["endpoint"])

# âœ… Endpoint pour Prometheus
@app.route('/metrics', methods=['GET'])
def metrics():
    return generate_latest(), 200, {'Content-Type': CONTENT_TYPE_LATEST}

# âœ… Middleware pour mesurer la latence des requÃªtes
@app.before_request
def start_timer():
    request.start_time = time.time()

@app.after_request
def log_request(response):
    """ Log et enregistre les mÃ©triques des requÃªtes """
    if hasattr(request, "start_time"):
        request_latency = time.time() - request.start_time
        REQUEST_LATENCY.labels(endpoint=request.path).observe(request_latency)
        REQUEST_COUNT.labels(method=request.method, endpoint=request.path, http_status=response.status_code).inc()
    return response

# âœ… VÃ©rification si les modÃ¨les existent
model_dir = "data/models"
if not os.path.exists(model_dir) or not os.listdir(model_dir):
    logging.warning("âš ï¸ Aucun modÃ¨le trouvÃ©, l'API fonctionnera sans modÃ¨les.")
    models = {}
else:
    logging.info("ğŸ“¥ Chargement des modÃ¨les...")
    models = load_models(model_dir)
    logging.info("âœ… ModÃ¨les chargÃ©s avec succÃ¨s !")

# âœ… VÃ©rification des modÃ¨les chargÃ©s
sae = models.get("sae")
scaler = models.get("scaler")
threshold = models.get("threshold", 0.0045)  # Valeur par dÃ©faut
seuil_anomalies = 10  # ğŸ”¹ Seuil Ã  partir duquel une panne est imminente

if sae is None or scaler is None:
    logging.warning("âš ï¸ Le modÃ¨le SAE ou le scaler n'a pas Ã©tÃ© chargÃ©. VÃ©rifiez `models_loader.py` et `data/models/`.")

# âœ… Endpoint de vÃ©rification du bon fonctionnement de l'API
@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({"status": "OK", "message": "L'API est en ligne et fonctionne correctement."}), 200

# âœ… Endpoint sÃ©curisÃ© pour la prÃ©diction
@app.route('/predict_csv', methods=['POST'])
@swag_from("/app/swagger/predict_csv.yaml")
@api_key_required
def predict_from_csv():
    try:
        if 'file' not in request.files:
            return jsonify({"error": "Aucun fichier envoyÃ©"}), 400

        file = request.files['file']
        df = pd.read_csv(file)

        # VÃ©rification des colonnes requises
        required_columns = [
            "timestamp", "TP2", "TP3", "H1", "DV_pressure", "Reservoirs",
            "Oil_temperature", "Motor_current", "COMP", "DV_eletric",
            "Towers", "MPG", "LPS", "Pressure_switch", "Oil_level", "Caudal_impulses"
        ]

        if not set(required_columns).issubset(df.columns):
            return jsonify({"error": "Colonnes manquantes dans le fichier CSV"}), 400

        # VÃ©rification de scaler avant transformation
        if not hasattr(scaler, "feature_names_in_"):
            logging.error("âŒ Scaler mal initialisÃ© ou absent.")
            return jsonify({"error": "Scaler mal initialisÃ©."}), 500

        # Normalisation avec gestion des erreurs
        try:
            df[scaler.feature_names_in_] = scaler.transform(df[scaler.feature_names_in_])
        except Exception as e:
            logging.error(f"âŒ Erreur de normalisation : {e}")
            return jsonify({"error": "Erreur de normalisation des donnÃ©es"}), 500

        # PrÃ©diction
        try:
            reconstructed_values = sae.predict(df[scaler.feature_names_in_])
            df["error"] = np.mean(np.square(df[scaler.feature_names_in_] - reconstructed_values), axis=1)
            df["anomaly"] = df["error"] > threshold
        except Exception as e:
            logging.error(f"âŒ Erreur lors de la prÃ©diction : {e}")
            return jsonify({"error": "Erreur lors de la prÃ©diction"}), 500

        # DÃ©tection de panne imminente
        total_anomalies = int(df["anomaly"].sum())
        panne_imminente = "OUI" if total_anomalies >= seuil_anomalies else "NON"

        return jsonify({
            "total_anomalies_detectÃ©es": total_anomalies,
            "panne_imminente": panne_imminente
        }), 200

    except Exception as e:
        logging.error(f"âŒ Erreur interne du serveur : {e}")
        return jsonify({"error": "Erreur interne du serveur"}), 500

# âœ… DÃ©marrage du serveur
if __name__ == "__main__":
    logging.info("ğŸš€ DÃ©marrage du serveur Flask avec Waitress...")
    serve(app, host="0.0.0.0", port=5000)
