{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a55cb04",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center; font-family: Arial, sans-serif; color: #4CAF50;\">TSS - Temporal Similarity Search : POC complete DATASET WITH API - avec enregistrement fichier .json volumineux et fichier parquet petit (sur le local pas de API) mais trop lourd dans la recherche victorielle</h3>\n",
    "<ul style=\"font-family: Arial, sans-serif; font-size: 12pt; color: #333;\">\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b1d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02852f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aee60a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données chargées avec succes.\n"
     ]
    }
   ],
   "source": [
    "# ==================================================\n",
    "# 1. Chargement et préparation des données avec Dask\n",
    "# ==================================================\n",
    "\n",
    "# Charger les données en Dask DataFrame\n",
    "df_dask = dd.read_csv(\"../Datasources/MetroPT3_imputed_final.csv\", delimiter=\",\", decimal=\".\", parse_dates=['timestamp'])\n",
    "\n",
    "# Sélection des colonnes continues et catégoriques\n",
    "continuous_features  = [\"TP2\", \"DV_pressure\", \"Oil_temperature\", \"Motor_current\", \"Reservoirs\"]\n",
    "categorical_features = [\"COMP\", \"DV_eletric\", \"Towers\", \"LPS\", \"Pressure_switch\", \"Oil_level\", \"Caudal_impulses\"]\n",
    "columns_to_keep = [\"timestamp\", \"panne\"] + continuous_features + categorical_features\n",
    "\n",
    "# Filtrer les colonnes utiles\n",
    "df_dask = df_dask[columns_to_keep]\n",
    "\n",
    "print(\"Données chargées avec succes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f303c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrainement : 1102501 lignes\n",
      "Test : 739259 lignes\n"
     ]
    }
   ],
   "source": [
    "# =================================================\n",
    "# 2. Séparation des données en entrainement et test\n",
    "# =================================================\n",
    "###################################################################################################\n",
    "################ Train : Panne1 + Panne2 + Panne3                                      ############\n",
    "################ Test  : Panne4                                                        ############\n",
    "###################################################################################################\n",
    "################ Classe 0 : Pas de panne détectée                                      ############\n",
    "################ Classe 1 : En plein panne                                             ############                              \n",
    "################ Classe 2 : Panne prévue dans moins de 15 minutes                      ############ \n",
    "###################################################################################################\n",
    "################ 2020-04-17 23:30:00 -- 2020-04-17 23:44:50 : 0                        ############\n",
    "################ 2020-04-17 23:45:00 -- 2020-04-17 23:59:50 : 2                        ############\n",
    "################ 2020-04-18 00:00:00 -- 2020-04-18 00:15:00 : 1                        ############\n",
    "################ 2020-04-18 00:15:10 -- 2020-04-18 00:30:00 : 0                        ############\n",
    "################ 2020-04-18 00:30:10 -- 2020-04-18 00:45:00 : 2                        ############\n",
    "################ 2020-04-18 00:45:10 -- 2020-04-18 01:00:10 : 1                        ############\n",
    "################ 2020-04-18 01:00:20 -- 2020-04-18 01:15:10 : 0                        ############\n",
    "################ 2020-04-18 01:15:20 -- 2020-04-18 01:30:10 : 2                        ############\n",
    "################ 2020-04-18 01:30:20 -- 2020-04-18 01:45:20 : 1                        ############ \n",
    "################ 2020-04-18 01:45:30 -- 2020-04-18 02:00:20 : 0                        ############\n",
    "################ 2020-04-18 02:00:30 -- 2020-04-18 02:15:20 : 2                        ############\n",
    "################ 2020-04-18 02:15:30 -- 2020-04-18 02:30:30 : 1                        ############\n",
    "###################################################################################################\n",
    "\n",
    "\n",
    "# Liste des intervalles de pannes\n",
    "pannes = [\n",
    "    {'start': '2020-04-18 00:00:00', 'end': '2020-04-18 23:59:00'},\n",
    "    {'start': '2020-05-29 23:30:00', 'end': '2020-05-30 06:00:00'},\n",
    "    {'start': '2020-06-05 10:00:00', 'end': '2020-06-07 14:30:00'},\n",
    "    {'start': '2020-07-15 14:30:00', 'end': '2020-07-15 19:00:00'},\n",
    "         ]\n",
    "\n",
    "# Définition des périodes d'entrainement\n",
    "train_periods = [{'start': '2020-02-01 00:00:00', 'end': '2020-06-07 14:30:00'}]\n",
    "test_periods  = [{'start': '2020-06-07 14:30:10', 'end': '2020-09-01 03:59:50'}]\n",
    "\n",
    "# Séparer les données d'entrainement et de test\n",
    "train_df = dd.concat([\n",
    "    df_dask[(df_dask['timestamp'] >= period['start']) & (df_dask['timestamp'] <= period['end'])]\n",
    "    for period in train_periods\n",
    "])\n",
    "\n",
    "test_df = dd.concat([\n",
    "    df_dask[(df_dask['timestamp'] >= period['start']) & (df_dask['timestamp'] <= period['end'])]\n",
    "    for period in test_periods\n",
    "])\n",
    "\n",
    "# Regrouper les partitions pour éviter les coupures temporelles\n",
    "train_df = train_df.repartition(npartitions=4).reset_index(drop=True)\n",
    "test_df = test_df.repartition(npartitions=1).reset_index(drop=True)\n",
    "\n",
    "# Calcul du nombre de lignes\n",
    "print(f\"Entrainement : {train_df.shape[0].compute()} lignes\")\n",
    "print(f\"Test : {test_df.shape[0].compute()} lignes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac64a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Génération des fenêtres d'entrainement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Création des fenêtres: 100%|████████████████████████████████████████████████| 1102496/1102496 [21:26<00:00, 856.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Génération des fenêtres de test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Création des fenêtres: 100%|██████████████████████████████████████████████████| 739254/739254 [14:27<00:00, 852.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de fenêtres d'entrainement : 1102496\n",
      "Nombre de fenêtres de test : 739254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# 3. Génération des fenêtres glissantes avec Dask\n",
    "# ===============================================\n",
    "\n",
    "window_size = 6  # Taille de la fenêtre\n",
    "step_size = 1      # Pas de glissement\n",
    "\n",
    "def create_windows(df):\n",
    "    windows = []\n",
    "    df = df.compute()  # Convertir en Pandas pour traitement en mémoire\n",
    "    for i in tqdm(range(0, len(df) - window_size + 1, step_size), desc=\"Création des fenêtres\"):\n",
    "        window_data = df.iloc[i:i + window_size][continuous_features].values.flatten()\n",
    "        panne_value = df.iloc[i:i + window_size][\"panne\"].mode()[0]  # Classe majoritaire\n",
    "        windows.append({\n",
    "            \"index\": i,\n",
    "            \"timestamp\": df.iloc[i][\"timestamp\"],\n",
    "            \"sensor_data\": window_data.tolist(),\n",
    "            \"panne\": panne_value\n",
    "        })\n",
    "    return windows\n",
    "\n",
    "# Génération des fenêtres d'entrainement et de test\n",
    "print(\"Génération des fenêtres d'entrainement...\")\n",
    "train_windows = create_windows(train_df)\n",
    "\n",
    "print(\"Génération des fenêtres de test...\")\n",
    "test_windows = create_windows(test_df)\n",
    "\n",
    "print(f\"Nombre de fenêtres d'entrainement : {len(train_windows)}\")\n",
    "print(f\"Nombre de fenêtres de test : {len(test_windows)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c63bc214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premier vecteur d'entrainement :\n",
      "[-0.0120000000000004, -0.0240000000000009, 53.60000000000001, 0.04, 9.358, -0.0139999999999993, -0.0219999999999984, 53.67500000000001, 0.04, 9.348, -0.0120000000000004, -0.0219999999999984, 53.60000000000001, 0.0424999999999995, 9.338, -0.0120000000000004, -0.0219999999999984, 53.42499999999998, 0.04, 9.328, -0.0120000000000004, -0.0219999999999984, 53.47500000000001, 0.04, 9.318, -0.0120000000000004, -0.0240000000000009, 53.49999999999999, 0.04, 9.308]\n"
     ]
    }
   ],
   "source": [
    "# Afficher le premier vecteur d'entrainement\n",
    "first_train_vector = train_windows[0][\"sensor_data\"]\n",
    "print(\"Premier vecteur d'entrainement :\")\n",
    "print(first_train_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "793c5eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vecteurs d'entrainement sauvegardés au format Parquet.\n",
      "Temps d'exécution: 41.91 secondes\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 4. Sauvegarde des fenêtres en format Parquet pour utilisation future avec Dask\n",
    "# ==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Démarrer le chronometre\n",
    "start_time = time.time()\n",
    "\n",
    "# Convertir les données en JSON string pour les sauvegarder correctement dans Parquet\n",
    "train_df_windows = pd.DataFrame(train_windows)\n",
    "train_df_windows[\"sensor_data\"] = train_df_windows[\"sensor_data\"].apply(json.dumps)\n",
    "\n",
    "# Convertir en DataFrame Dask avec plusieurs partitions\n",
    "train_df_dask = dd.from_pandas(train_df_windows, npartitions=4)\n",
    "\n",
    "# Sauvegarde en Parquet sans l'argument index=False\n",
    "train_df_dask.to_parquet(\"Generated_Files/Model_1_TSS_POC_DASK_train_vectors.parquet\")\n",
    "\n",
    "print(\"Vecteurs d'entrainement sauvegardés au format Parquet.\")\n",
    "\n",
    "# Arrêter le chronometre\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculer le temps d'exécution\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Temps d'exécution: {execution_time:.2f} secondes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80a981a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données d'entrainement chargées : (1102496, 30)\n",
      "Nombre de partitions dans le DataFrame Dask : 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sensor_data</th>\n",
       "      <th>panne</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-02-01 00:00:00</td>\n",
       "      <td>[-0.0120000000000004, -0.0240000000000009, 53....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-01 00:00:10</td>\n",
       "      <td>[-0.0139999999999993, -0.0219999999999984, 53....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-02-01 00:00:20</td>\n",
       "      <td>[-0.0120000000000004, -0.0219999999999984, 53....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-02-01 00:00:30</td>\n",
       "      <td>[-0.0120000000000004, -0.0219999999999984, 53....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-02-01 00:00:40</td>\n",
       "      <td>[-0.0120000000000004, -0.0219999999999984, 53....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index           timestamp  \\\n",
       "0      0 2020-02-01 00:00:00   \n",
       "1      1 2020-02-01 00:00:10   \n",
       "2      2 2020-02-01 00:00:20   \n",
       "3      3 2020-02-01 00:00:30   \n",
       "4      4 2020-02-01 00:00:40   \n",
       "\n",
       "                                         sensor_data  panne  \n",
       "0  [-0.0120000000000004, -0.0240000000000009, 53....      0  \n",
       "1  [-0.0139999999999993, -0.0219999999999984, 53....      0  \n",
       "2  [-0.0120000000000004, -0.0219999999999984, 53....      0  \n",
       "3  [-0.0120000000000004, -0.0219999999999984, 53....      0  \n",
       "4  [-0.0120000000000004, -0.0219999999999984, 53....      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution: 48.17 secondes\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 5.  Chargement des vecteurs d'entrainement pour prédiction\n",
    "# ==========================================================\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Démarrer le chronometre\n",
    "start_time = time.time()\n",
    "\n",
    "# Charger les données d'entrainement sauvegardées\n",
    "train_df_loaded = dd.read_parquet(\"Generated_Files/Model_1_TSS_POC_DASK_train_vectors.parquet\")\n",
    "\n",
    "# Convertir la colonne 'sensor_data' de JSON string en liste pour la recherche vectorielle\n",
    "train_df_loaded[\"sensor_data\"] = train_df_loaded[\"sensor_data\"].apply(json.loads, meta=(\"sensor_data\", \"object\"))\n",
    "\n",
    "# Transformation en format utilisable pour la recherche vectorielle\n",
    "X_train_loaded = np.vstack(train_df_loaded[\"sensor_data\"].compute())\n",
    "y_train_loaded = train_df_loaded[\"panne\"].compute().values\n",
    "\n",
    "print(f\"Données d'entrainement chargées : {X_train_loaded.shape}\")\n",
    "\n",
    "# Vérification du nombre de partitions\n",
    "print(f\"Nombre de partitions dans le DataFrame Dask : {train_df_loaded.npartitions}\")\n",
    "\n",
    "\n",
    "# Afficher un extrait des données chargées\n",
    "display(train_df_loaded.head())\n",
    "\n",
    "# Arrêter le chronometre\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'exécution: {end_time - start_time:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "889c9171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes dans le test : 739259\n",
      "Nombre de partitions dans le test : 1\n"
     ]
    }
   ],
   "source": [
    "# Préparation du nouvel ensemble de test\n",
    "\n",
    "test_df = dd.concat([\n",
    "    df_dask[(df_dask['timestamp'] >= period['start']) & (df_dask['timestamp'] <= period['end'])]\n",
    "    for period in test_periods\n",
    "])\n",
    "\n",
    "# Regrouper les partitions pour éviter les coupures temporelles\n",
    "test_df = test_df.repartition(npartitions=1).reset_index(drop=True)\n",
    "\n",
    "# Vérification du nombre de lignes et partitions\n",
    "print(f\"Nombre de lignes dans le test : {test_df.shape[0].compute()}\")\n",
    "print(f\"Nombre de partitions dans le test : {test_df.npartitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb441f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Génération des fenêtres de test...\n",
      "Nombre de fenêtres générées : 739254\n",
      "Temps d'exécution: 831.23 secondes\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# 6. Génération des fenêtres glissantes pour l'ensemble de test avec Dask\n",
    "# =======================================================================\n",
    "\n",
    "# Démarrer le chronometre\n",
    "start_time = time.time()\n",
    "\n",
    "# Fonction pour créer des fenêtres de test\n",
    "window_size = 6    # Taille de la fenêtre\n",
    "step_size = 1      # Pas de glissement\n",
    "\n",
    "def create_windows(df):\n",
    "    windows = []\n",
    "    df = df.compute()  # Convertir en Pandas pour traitement en mémoire\n",
    "    for i in range(0, len(df) - window_size + 1, step_size):\n",
    "        window_data = df.iloc[i:i + window_size][continuous_features].values.flatten()\n",
    "        panne_value = df.iloc[i:i + window_size][\"panne\"].mode()[0]  # Classe majoritaire\n",
    "        windows.append({\n",
    "            \"index\": i,\n",
    "            \"timestamp\": df.iloc[i][\"timestamp\"],\n",
    "            \"sensor_data\": window_data.tolist(),\n",
    "            \"panne\": panne_value\n",
    "        })\n",
    "    return windows\n",
    "\n",
    "# Générer les fenêtres de test\n",
    "print(\"Génération des fenêtres de test...\")\n",
    "test_windows = create_windows(test_df)\n",
    "\n",
    "print(f\"Nombre de fenêtres générées : {len(test_windows)}\")\n",
    "\n",
    "# Arrêter le chronometre\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'exécution: {end_time - start_time:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c028383f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prédictions en cours: 100%|█████████████████████████████████████████████████| 739254/739254 [23:56:26<00:00,  8.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédictions sauvegardées avec succes.\n",
      "Temps d'exécution: 86198.52 secondes\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# 7. Prédiction sur l'ensemble de test\n",
    "# ====================================\n",
    "\n",
    "# Démarrer le chronometre\n",
    "start_time = time.time()\n",
    "\n",
    "# Fonction de recherche des k plus proches voisins en utilisant la distance euclidienne\n",
    "def find_similar_windows(test_vector, train_vectors, train_labels, k=5):\n",
    "    # Calculer les distances entre le vecteur de test et les vecteurs d'entrainement\n",
    "    distances = cdist([test_vector], train_vectors, metric=\"euclidean\")\n",
    "    \n",
    "    # Trouver les indices des k plus proches voisins\n",
    "    nearest_indices = np.argsort(distances, axis=1)[:, :k]\n",
    "\n",
    "    # Prédiction par vote majoritaire\n",
    "    similar_pannes = train_labels[nearest_indices.flatten()]\n",
    "    predicted_panne = np.bincount(similar_pannes).argmax()\n",
    "\n",
    "    return predicted_panne\n",
    "\n",
    "# Appliquer la prédiction sur l'ensemble de test\n",
    "predictions = []\n",
    "for test_sample in tqdm(test_windows, desc=\"Prédictions en cours\"):\n",
    "    test_vector = test_sample[\"sensor_data\"]\n",
    "    predicted_panne = find_similar_windows(test_vector, X_train_loaded, y_train_loaded)\n",
    "    \n",
    "    predictions.append({\n",
    "        \"timestamp\": test_sample[\"timestamp\"],\n",
    "        \"actual_panne\": test_sample[\"panne\"],\n",
    "        \"predicted_panne\": predicted_panne\n",
    "    })\n",
    "\n",
    "# Convertir en DataFrame Pandas puis en Dask\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_dask = dd.from_pandas(predictions_df, npartitions=1)\n",
    "\n",
    "# Sauvegarder les prédictions en format Parquet\n",
    "predictions_dask.to_parquet(\"Generated_Files/test_predictions.parquet\")\n",
    "print(\"Prédictions sauvegardées avec succes.\")\n",
    "\n",
    "# Arrêter le chronometre\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'exécution: {end_time - start_time:.2f} secondes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0624498b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classe</th>\n",
       "      <th>Observations réelles</th>\n",
       "      <th>Prédictions correctes</th>\n",
       "      <th>Remarque</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0 (Pas de panne)</td>\n",
       "      <td>737454</td>\n",
       "      <td>737141</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 (En pleine panne)</td>\n",
       "      <td>1621</td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 (Panne imminente)</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>Le modele n'a jamais correctement prédit cette...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Classe  Observations réelles  Prédictions correctes  \\\n",
       "0     0 (Pas de panne)                737454                 737141   \n",
       "1  1 (En pleine panne)                  1621                     10   \n",
       "2  2 (Panne imminente)                   179                      0   \n",
       "\n",
       "                                            Remarque  \n",
       "0                                                     \n",
       "1                                                     \n",
       "2  Le modele n'a jamais correctement prédit cette...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger le fichier CSV\n",
    "df_predictions = pd.read_csv(\n",
    "    \"Generated_Files/final_predictions.csv\",\n",
    "    header=None,\n",
    "    names=[\"timestamp\", \"panne_réelle\", \"panne_prédite\"],\n",
    "    dtype={\"timestamp\": str, \"panne_réelle\": str, \"panne_prédite\": str},  # Charger d'abord en str\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "# Convertir les colonnes en type numérique pour éliminer les incohérences\n",
    "df_predictions[\"panne_réelle\"] = pd.to_numeric(df_predictions[\"panne_réelle\"], errors='coerce')\n",
    "df_predictions[\"panne_prédite\"] = pd.to_numeric(df_predictions[\"panne_prédite\"], errors='coerce')\n",
    "\n",
    "# Supprimer les valeurs nulles si nécessaire\n",
    "df_predictions.dropna(inplace=True)\n",
    "\n",
    "# Calcul des répartitions des classes réelles et prédites\n",
    "class_counts_real = df_predictions[\"panne_réelle\"].value_counts().sort_index()\n",
    "\n",
    "# Ajouter une colonne de vérification pour identifier les prédictions correctes\n",
    "df_predictions[\"correct\"] = df_predictions[\"panne_réelle\"] == df_predictions[\"panne_prédite\"]\n",
    "\n",
    "# Calcul du nombre de bonnes prédictions par classe\n",
    "correct_predictions = df_predictions[df_predictions[\"correct\"]].groupby(\"panne_réelle\").size().reindex(class_counts_real.index, fill_value=0)\n",
    "\n",
    "# Création du tableau récapitulatif sans la colonne \"Prédictions\"\n",
    "recap_data_corrected = {\n",
    "    \"Classe\": [\n",
    "        f\"{int(class_label)} (Pas de panne)\" if class_label == 0 else\n",
    "        f\"{int(class_label)} (En pleine panne)\" if class_label == 1 else\n",
    "        f\"{int(class_label)} (Panne imminente)\"\n",
    "        for class_label in class_counts_real.index\n",
    "    ],\n",
    "    \"Observations réelles\": class_counts_real.values,\n",
    "    \"Prédictions correctes\": correct_predictions.values,\n",
    "    \"Remarque\": [\n",
    "        \"\" if correct_predictions[class_label] > 0 else \"Le modele n'a jamais correctement prédit cette classe.\"\n",
    "        for class_label in class_counts_real.index\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Création du DataFrame corrigé\n",
    "recap_df_corrected = pd.DataFrame(recap_data_corrected)\n",
    "\n",
    "# Affichage du tableau récapitulatif\n",
    "recap_df_corrected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "669d2158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usermine\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactitude (Accuracy) : 1.00\n",
      "Précision : 1.00\n",
      "Rappel (Recall) : 1.00\n",
      "Score F1 : 1.00\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 8. Évaluation des performances du modele\n",
    "# ========================================\n",
    "\n",
    "# Charger les prédictions sauvegardées pour analyse\n",
    "predictions_df = dd.read_parquet(\"Generated_Files/test_predictions.parquet\").compute()\n",
    "\n",
    "# Calcul des métriques de performance\n",
    "y_true = predictions_df[\"actual_panne\"].values\n",
    "y_pred = predictions_df[\"predicted_panne\"].values\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "# Afficher les résultats\n",
    "print(f\"Exactitude (Accuracy) : {accuracy:.2f}\")\n",
    "print(f\"Précision : {precision:.2f}\")\n",
    "print(f\"Rappel (Recall) : {recall:.2f}\")\n",
    "print(f\"Score F1 : {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6983c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les prédictions finales ont été enregistrées avec succes.\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde au format CSV pour analyse ultérieure\n",
    "predictions_df.to_csv(\"Generated_Files/final_predictions.csv\", index=False)\n",
    "\n",
    "print(\"Les prédictions finales ont été enregistrées avec succes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bc4a7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Période d'entrainement : 2020-02-01 00:00:00 -> 2020-06-07 14:29:10\n",
      "Période de test        : 2020-06-07 14:30:10 -> 2020-09-01 03:59:00\n",
      "Pas de chevauchement détecté.\n"
     ]
    }
   ],
   "source": [
    "#  1 : Vérification de la séparation des données d'entrainement et de test.\n",
    "\n",
    "# Vérification des plages de timestamps\n",
    "train_start = train_min_timestamp\n",
    "train_end = train_max_timestamp\n",
    "test_start = test_min_timestamp\n",
    "test_end = test_max_timestamp\n",
    "\n",
    "print(f\"Période d'entrainement : {train_start} -> {train_end}\")\n",
    "print(f\"Période de test        : {test_start} -> {test_end}\")\n",
    "\n",
    "# Vérification d'un chevauchement éventuel\n",
    "if train_end >= test_start:\n",
    "    print(\"Il y a un chevauchement entre l'entrainement et le test. Vérifiez la séparation des données.\")\n",
    "else:\n",
    "    print(\"Pas de chevauchement détecté.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd812d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistiques pour Entrainement:\n",
      "Min: -0.0320\n",
      "Max: 83.1250\n",
      "Moyenne: 14.0246\n",
      "Médiane: 1.4520\n",
      "Écart-type: 22.9968\n",
      "\n",
      "\n",
      "Statistiques pour Test:\n",
      "Min: -0.0320\n",
      "Max: 89.0500\n",
      "Moyenne: 14.9613\n",
      "Médiane: 3.1548\n",
      "Écart-type: 24.6296\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 : Vérification des statistiques descriptives sur les données d'entrainement et de test\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Extraction des valeurs des capteurs des fenêtres d'entrainement et de test\n",
    "train_values = np.array([sample[\"sensor_data\"] for sample in train_windows]).flatten()\n",
    "test_values = np.array([sample[\"sensor_data\"] for sample in test_windows]).flatten()\n",
    "\n",
    "# Calcul des statistiques descriptives\n",
    "def get_statistics(values, dataset_name):\n",
    "    stats = {\n",
    "        \"Min\": np.min(values),\n",
    "        \"Max\": np.max(values),\n",
    "        \"Moyenne\": np.mean(values),\n",
    "        \"Médiane\": np.median(values),\n",
    "        \"Écart-type\": np.std(values)\n",
    "    }\n",
    "    print(f\"Statistiques pour {dataset_name}:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Affichage des statistiques pour l'entrainement et le test\n",
    "get_statistics(train_values, \"Entrainement\")\n",
    "get_statistics(test_values, \"Test\")\n",
    "\n",
    "# Ces différences peuvent expliquer pourquoi le modele obtient des performances parfaites. \n",
    "# Si les données de test ne sont pas bien représentées par celles d'entrainement (notamment pour les valeurs extrêmes),\n",
    "# le modele pourrait ne pas être confronté à des cas difficiles pendant le test. \n",
    "# Cela peut indiquer un probleme potentiel de surajustement (overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e240ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution des classes dans l'entrainement : Counter({0: 1072084, 1: 29875, 2: 537})\n",
      "Distribution des classes dans le test : Counter({0: 737454, 1: 1621, 2: 179})\n"
     ]
    }
   ],
   "source": [
    "# 3 : Vérification de la distribution des classes\n",
    "from collections import Counter\n",
    "\n",
    "# Comptage des occurrences des classes dans l'entrainement et le test\n",
    "train_classes = [sample[\"panne\"] for sample in train_windows]\n",
    "test_classes = [sample[\"panne\"] for sample in test_windows]\n",
    "\n",
    "# Affichage de la distribution des classes\n",
    "print(\"Distribution des classes dans l'entrainement :\", Counter(train_classes))\n",
    "print(\"Distribution des classes dans le test :\", Counter(test_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b6d31831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de valeurs manquantes dans l'ensemble d'entrainement : 0\n",
      "Nombre de valeurs manquantes dans l'ensemble de test : 0\n"
     ]
    }
   ],
   "source": [
    "# 4 : Vérification de l'imputation des données\n",
    "    \n",
    "# Vérification des valeurs imputées dans l'entrainement et le test\n",
    "train_missing = sum(np.isnan(train_values))\n",
    "test_missing = sum(np.isnan(test_values))\n",
    "\n",
    "print(f\"Nombre de valeurs manquantes dans l'ensemble d'entrainement : {train_missing}\")\n",
    "print(f\"Nombre de valeurs manquantes dans l'ensemble de test : {test_missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "632064ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Différences de corrélation entre l'entrainement et le test :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.491452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538979</td>\n",
       "      <td>0.318475</td>\n",
       "      <td>0.132623</td>\n",
       "      <td>0.521240</td>\n",
       "      <td>0.772223</td>\n",
       "      <td>0.533276</td>\n",
       "      <td>0.324220</td>\n",
       "      <td>0.128068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520987</td>\n",
       "      <td>0.774669</td>\n",
       "      <td>0.533786</td>\n",
       "      <td>0.324264</td>\n",
       "      <td>0.128468</td>\n",
       "      <td>0.522750</td>\n",
       "      <td>0.776426</td>\n",
       "      <td>0.534321</td>\n",
       "      <td>0.324946</td>\n",
       "      <td>0.129524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.455603</td>\n",
       "      <td>0.776426</td>\n",
       "      <td>0.546232</td>\n",
       "      <td>0.302767</td>\n",
       "      <td>0.132693</td>\n",
       "      <td>0.451682</td>\n",
       "      <td>0.774669</td>\n",
       "      <td>0.542598</td>\n",
       "      <td>0.301035</td>\n",
       "      <td>0.130951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.449399</td>\n",
       "      <td>0.772223</td>\n",
       "      <td>0.536552</td>\n",
       "      <td>0.299916</td>\n",
       "      <td>0.128415</td>\n",
       "      <td>0.491452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538975</td>\n",
       "      <td>0.318474</td>\n",
       "      <td>0.132623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.451682</td>\n",
       "      <td>0.774669</td>\n",
       "      <td>0.542599</td>\n",
       "      <td>0.301036</td>\n",
       "      <td>0.130951</td>\n",
       "      <td>0.450513</td>\n",
       "      <td>0.774082</td>\n",
       "      <td>0.540307</td>\n",
       "      <td>0.300416</td>\n",
       "      <td>0.130122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.491452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538976</td>\n",
       "      <td>0.318474</td>\n",
       "      <td>0.132623</td>\n",
       "      <td>0.521239</td>\n",
       "      <td>0.772223</td>\n",
       "      <td>0.533273</td>\n",
       "      <td>0.324219</td>\n",
       "      <td>0.128068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.450513</td>\n",
       "      <td>0.774082</td>\n",
       "      <td>0.540307</td>\n",
       "      <td>0.300416</td>\n",
       "      <td>0.130122</td>\n",
       "      <td>0.448943</td>\n",
       "      <td>0.771949</td>\n",
       "      <td>0.538247</td>\n",
       "      <td>0.299827</td>\n",
       "      <td>0.129198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.521239</td>\n",
       "      <td>0.772223</td>\n",
       "      <td>0.533274</td>\n",
       "      <td>0.324219</td>\n",
       "      <td>0.128068</td>\n",
       "      <td>0.520472</td>\n",
       "      <td>0.771950</td>\n",
       "      <td>0.532802</td>\n",
       "      <td>0.324035</td>\n",
       "      <td>0.128256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.449399</td>\n",
       "      <td>0.772223</td>\n",
       "      <td>0.536554</td>\n",
       "      <td>0.299917</td>\n",
       "      <td>0.128415</td>\n",
       "      <td>0.491452</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538978</td>\n",
       "      <td>0.318475</td>\n",
       "      <td>0.132623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520517</td>\n",
       "      <td>0.774082</td>\n",
       "      <td>0.533543</td>\n",
       "      <td>0.324082</td>\n",
       "      <td>0.128343</td>\n",
       "      <td>0.520987</td>\n",
       "      <td>0.774669</td>\n",
       "      <td>0.533785</td>\n",
       "      <td>0.324264</td>\n",
       "      <td>0.128468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.448943</td>\n",
       "      <td>0.771949</td>\n",
       "      <td>0.538248</td>\n",
       "      <td>0.299827</td>\n",
       "      <td>0.129198</td>\n",
       "      <td>0.449399</td>\n",
       "      <td>0.772223</td>\n",
       "      <td>0.536554</td>\n",
       "      <td>0.299916</td>\n",
       "      <td>0.128415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520472</td>\n",
       "      <td>0.771950</td>\n",
       "      <td>0.532803</td>\n",
       "      <td>0.324036</td>\n",
       "      <td>0.128256</td>\n",
       "      <td>0.520517</td>\n",
       "      <td>0.774082</td>\n",
       "      <td>0.533542</td>\n",
       "      <td>0.324082</td>\n",
       "      <td>0.128343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.298339</td>\n",
       "      <td>0.533276</td>\n",
       "      <td>0.017492</td>\n",
       "      <td>0.212965</td>\n",
       "      <td>0.348765</td>\n",
       "      <td>0.289260</td>\n",
       "      <td>0.538978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207572</td>\n",
       "      <td>0.368510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354179</td>\n",
       "      <td>0.540307</td>\n",
       "      <td>0.021187</td>\n",
       "      <td>0.242842</td>\n",
       "      <td>0.347118</td>\n",
       "      <td>0.366133</td>\n",
       "      <td>0.542598</td>\n",
       "      <td>0.024042</td>\n",
       "      <td>0.249422</td>\n",
       "      <td>0.345884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.289261</td>\n",
       "      <td>0.538979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207575</td>\n",
       "      <td>0.368513</td>\n",
       "      <td>0.327301</td>\n",
       "      <td>0.536554</td>\n",
       "      <td>0.017492</td>\n",
       "      <td>0.228454</td>\n",
       "      <td>0.348899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366133</td>\n",
       "      <td>0.542599</td>\n",
       "      <td>0.024041</td>\n",
       "      <td>0.249425</td>\n",
       "      <td>0.345886</td>\n",
       "      <td>0.372235</td>\n",
       "      <td>0.546232</td>\n",
       "      <td>0.024323</td>\n",
       "      <td>0.252845</td>\n",
       "      <td>0.347991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.284328</td>\n",
       "      <td>0.532805</td>\n",
       "      <td>0.018980</td>\n",
       "      <td>0.205179</td>\n",
       "      <td>0.347830</td>\n",
       "      <td>0.298338</td>\n",
       "      <td>0.533275</td>\n",
       "      <td>0.017492</td>\n",
       "      <td>0.212962</td>\n",
       "      <td>0.348762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.341159</td>\n",
       "      <td>0.538246</td>\n",
       "      <td>0.018980</td>\n",
       "      <td>0.235823</td>\n",
       "      <td>0.348122</td>\n",
       "      <td>0.354179</td>\n",
       "      <td>0.540306</td>\n",
       "      <td>0.021187</td>\n",
       "      <td>0.242840</td>\n",
       "      <td>0.347115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.272226</td>\n",
       "      <td>0.533544</td>\n",
       "      <td>0.021187</td>\n",
       "      <td>0.198140</td>\n",
       "      <td>0.346623</td>\n",
       "      <td>0.284327</td>\n",
       "      <td>0.532804</td>\n",
       "      <td>0.018980</td>\n",
       "      <td>0.205176</td>\n",
       "      <td>0.347827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327299</td>\n",
       "      <td>0.536552</td>\n",
       "      <td>0.017493</td>\n",
       "      <td>0.228446</td>\n",
       "      <td>0.348891</td>\n",
       "      <td>0.341158</td>\n",
       "      <td>0.538245</td>\n",
       "      <td>0.018980</td>\n",
       "      <td>0.235820</td>\n",
       "      <td>0.348119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "1   0.491452  0.000000  0.538979  0.318475  0.132623  0.521240  0.772223   \n",
       "26  0.455603  0.776426  0.546232  0.302767  0.132693  0.451682  0.774669   \n",
       "21  0.451682  0.774669  0.542599  0.301036  0.130951  0.450513  0.774082   \n",
       "16  0.450513  0.774082  0.540307  0.300416  0.130122  0.448943  0.771949   \n",
       "6   0.449399  0.772223  0.536554  0.299917  0.128415  0.491452  0.000000   \n",
       "11  0.448943  0.771949  0.538248  0.299827  0.129198  0.449399  0.772223   \n",
       "7   0.298339  0.533276  0.017492  0.212965  0.348765  0.289260  0.538978   \n",
       "2   0.289261  0.538979  0.000000  0.207575  0.368513  0.327301  0.536554   \n",
       "12  0.284328  0.532805  0.018980  0.205179  0.347830  0.298338  0.533275   \n",
       "17  0.272226  0.533544  0.021187  0.198140  0.346623  0.284327  0.532804   \n",
       "\n",
       "          7         8         9   ...        20        21        22        23  \\\n",
       "1   0.533276  0.324220  0.128068  ...  0.520987  0.774669  0.533786  0.324264   \n",
       "26  0.542598  0.301035  0.130951  ...  0.449399  0.772223  0.536552  0.299916   \n",
       "21  0.540307  0.300416  0.130122  ...  0.491452  0.000000  0.538976  0.318474   \n",
       "16  0.538247  0.299827  0.129198  ...  0.521239  0.772223  0.533274  0.324219   \n",
       "6   0.538978  0.318475  0.132623  ...  0.520517  0.774082  0.533543  0.324082   \n",
       "11  0.536554  0.299916  0.128415  ...  0.520472  0.771950  0.532803  0.324036   \n",
       "7   0.000000  0.207572  0.368510  ...  0.354179  0.540307  0.021187  0.242842   \n",
       "2   0.017492  0.228454  0.348899  ...  0.366133  0.542599  0.024041  0.249425   \n",
       "12  0.017492  0.212962  0.348762  ...  0.341159  0.538246  0.018980  0.235823   \n",
       "17  0.018980  0.205176  0.347827  ...  0.327299  0.536552  0.017493  0.228446   \n",
       "\n",
       "          24        25        26        27        28        29  \n",
       "1   0.128468  0.522750  0.776426  0.534321  0.324946  0.129524  \n",
       "26  0.128415  0.491452  0.000000  0.538975  0.318474  0.132623  \n",
       "21  0.132623  0.521239  0.772223  0.533273  0.324219  0.128068  \n",
       "16  0.128068  0.520472  0.771950  0.532802  0.324035  0.128256  \n",
       "6   0.128343  0.520987  0.774669  0.533785  0.324264  0.128468  \n",
       "11  0.128256  0.520517  0.774082  0.533542  0.324082  0.128343  \n",
       "7   0.347118  0.366133  0.542598  0.024042  0.249422  0.345884  \n",
       "2   0.345886  0.372235  0.546232  0.024323  0.252845  0.347991  \n",
       "12  0.348122  0.354179  0.540306  0.021187  0.242840  0.347115  \n",
       "17  0.348891  0.341158  0.538245  0.018980  0.235820  0.348119  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5 : Corrélation entre les ensembles\n",
    "\n",
    "# Convertir les données en DataFrame pour calculer les corrélations\n",
    "train_df = pd.DataFrame([sample[\"sensor_data\"] for sample in train_windows])\n",
    "test_df = pd.DataFrame([sample[\"sensor_data\"] for sample in test_windows])\n",
    "\n",
    "# Calcul des corrélations\n",
    "train_corr = train_df.corr()\n",
    "test_corr = test_df.corr()\n",
    "\n",
    "# Comparaison des corrélations (différence absolue)\n",
    "corr_diff = (train_corr - test_corr).abs()\n",
    "\n",
    "# Afficher les variables ayant des différences de corrélation élevées\n",
    "print(\"Différences de corrélation entre l'entrainement et le test :\")\n",
    "display(corr_diff.sort_values(by=corr_diff.columns[0], ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e127c332",
   "metadata": {},
   "source": [
    "# TEST AVEC MINI DATASET DE TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "73b380c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes dans le test : 271\n",
      "Nombre de partitions dans le test : 1\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 1. Préparation du nouvel ensemble de test\n",
    "# =========================================\n",
    "\n",
    "# Tester avec un mini dataset de l'ensemble de Train, on est sensé d,avoir les metriques a 100%\n",
    "\n",
    "pannes = [\n",
    "    {'start': '2020-04-18 00:00:00', 'end': '2020-04-18 23:59:00'},\n",
    "    {'start': '2020-05-29 23:30:00', 'end': '2020-05-30 06:00:00'},\n",
    "    {'start': '2020-06-05 10:00:00', 'end': '2020-06-07 14:30:00'},\n",
    "    {'start': '2020-07-15 14:30:00', 'end': '2020-07-15 19:00:00'},\n",
    "         ]\n",
    "\n",
    "test2_periods  = [{'start': '2020-05-29 23:00:00', 'end': '2020-05-29 23:45:00'}]\n",
    "\n",
    "\n",
    "test2_df = dd.concat([\n",
    "    df_dask[(df_dask['timestamp'] >= period['start']) & (df_dask['timestamp'] <= period['end'])]\n",
    "    for period in test2_periods\n",
    "])\n",
    "\n",
    "# Regrouper les partitions pour éviter les coupures temporelles\n",
    "test2_df = test2_df.repartition(npartitions=1).reset_index(drop=True)\n",
    "\n",
    "# Vérification du nombre de lignes et partitions\n",
    "print(f\"Nombre de lignes dans le test : {test2_df.shape[0].compute()}\")\n",
    "print(f\"Nombre de partitions dans le test : {test2_df.npartitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "60d193ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Génération des fenêtres de test...\n",
      "Nombre de fenêtres générées : 266\n",
      "Temps d'exécution: 6.30 secondes\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# 2. Génération des fenêtres glissantes pour l'ensemble de test avec Dask\n",
    "# =======================================================================\n",
    "\n",
    "# Démarrer le chronometre\n",
    "start_time = time.time()\n",
    "\n",
    "# Fonction pour créer des fenêtres de test\n",
    "window_size = 6    # Taille de la fenêtre\n",
    "step_size = 1      # Pas de glissement\n",
    "\n",
    "def create_windows(df):\n",
    "    windows = []\n",
    "    df = df.compute()  # Convertir en Pandas pour traitement en mémoire\n",
    "    for i in range(0, len(df) - window_size + 1, step_size):\n",
    "        window_data = df.iloc[i:i + window_size][continuous_features].values.flatten()\n",
    "        panne_value = df.iloc[i:i + window_size][\"panne\"].mode()[0]  # Classe majoritaire\n",
    "        windows.append({\n",
    "            \"index\": i,\n",
    "            \"timestamp\": df.iloc[i][\"timestamp\"],\n",
    "            \"sensor_data\": window_data.tolist(),\n",
    "            \"panne\": panne_value\n",
    "        })\n",
    "    return windows\n",
    "\n",
    "# Générer les fenêtres de test\n",
    "print(\"Génération des fenêtres de test...\")\n",
    "test_windows = create_windows(test2_df)\n",
    "\n",
    "print(f\"Nombre de fenêtres générées : {len(test_windows)}\")\n",
    "\n",
    "# Arrêter le chronometre\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'exécution: {end_time - start_time:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bf0f1a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prédictions en cours:   0%|                                                                    | 0/266 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_sample \u001b[38;5;129;01min\u001b[39;00m tqdm(test_windows, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrédictions en cours\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     27\u001b[0m     test_vector \u001b[38;5;241m=\u001b[39m test_sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msensor_data\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 28\u001b[0m     predicted_panne \u001b[38;5;241m=\u001b[39m \u001b[43mfind_similar_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_loaded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_loaded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual_panne\u001b[39m\u001b[38;5;124m\"\u001b[39m: test_sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpanne\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_panne\u001b[39m\u001b[38;5;124m\"\u001b[39m: predicted_panne\n\u001b[0;32m     34\u001b[0m     })\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Convertir en DataFrame Pandas puis en Dask\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[78], line 19\u001b[0m, in \u001b[0;36mfind_similar_windows\u001b[1;34m(test_vector, train_vectors, train_labels, k)\u001b[0m\n\u001b[0;32m     16\u001b[0m nearest_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(distances, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, :k]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Prédiction par vote majoritaire\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m similar_pannes \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnearest_indices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     20\u001b[0m predicted_panne \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(similar_pannes)\u001b[38;5;241m.\u001b[39margmax()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predicted_panne\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# 7. Prédiction sur l'ensemble de test\n",
    "# ====================================\n",
    "\n",
    "# Démarrer le chronometre\n",
    "start_time = time.time()\n",
    "\n",
    "# Fonction de recherche des k plus proches voisins en utilisant la distance euclidienne\n",
    "def find_similar_windows(test_vector, train_vectors, train_labels, k=5):\n",
    "    # Calculer les distances entre le vecteur de test et les vecteurs d'entrainement\n",
    "    distances = cdist([test_vector], train_vectors, metric=\"euclidean\")\n",
    "    #distances = cdist([test_vector], train_vectors, metric=\"cosine\")\n",
    "    \n",
    "    # Trouver les indices des k plus proches voisins\n",
    "    # nearest_indices = np.argsort(distances, axis=1)[:, :k]\n",
    "    nearest_indices = np.argsort(distances, axis=1)[:, :k].astype(int)\n",
    "\n",
    "    # Prédiction par vote majoritaire\n",
    "    similar_pannes = train_labels[nearest_indices.flatten()]\n",
    "    predicted_panne = np.bincount(similar_pannes).argmax()\n",
    "\n",
    "    return predicted_panne\n",
    "\n",
    "# Appliquer la prédiction sur l'ensemble de test\n",
    "predictions = []\n",
    "for test_sample in tqdm(test_windows, desc=\"Prédictions en cours\"):\n",
    "    test_vector = test_sample[\"sensor_data\"]\n",
    "    predicted_panne = find_similar_windows(test_vector, X_train_loaded, y_train_loaded)\n",
    "    \n",
    "    predictions.append({\n",
    "        \"timestamp\": test_sample[\"timestamp\"],\n",
    "        \"actual_panne\": test_sample[\"panne\"],\n",
    "        \"predicted_panne\": predicted_panne\n",
    "    })\n",
    "\n",
    "# Convertir en DataFrame Pandas puis en Dask\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_dask = dd.from_pandas(predictions_df, npartitions=1)\n",
    "\n",
    "# Sauvegarder les prédictions en format Parquet\n",
    "# predictions_dask.to_parquet(\"Generated_Files/test_predictions.parquet\")\n",
    "# print(\"Prédictions sauvegardées avec succes.\")\n",
    "\n",
    "# Arrêter le chronometre\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'exécution: {end_time - start_time:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f71bd69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Les prédictions finales ont été enregistrées avec succes.\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde au format CSV pour analyse ultérieure\n",
    "#predictions_df.to_csv(\"Generated_Files/final2_predictions.csv\", index=False)\n",
    "\n",
    "#print(\"Les prédictions finales ont été enregistrées avec succes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "caa5f45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classe</th>\n",
       "      <th>Observations réelles</th>\n",
       "      <th>Prédictions correctes</th>\n",
       "      <th>Remarque</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0 (Pas de panne)</td>\n",
       "      <td>737454</td>\n",
       "      <td>737141</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 (En pleine panne)</td>\n",
       "      <td>1621</td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 (Panne imminente)</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>Le modele n'a jamais prédit cette classe.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Classe  Observations réelles  Prédictions correctes  \\\n",
       "0     0 (Pas de panne)                737454                 737141   \n",
       "1  1 (En pleine panne)                  1621                     10   \n",
       "2  2 (Panne imminente)                   179                      0   \n",
       "\n",
       "                                    Remarque  \n",
       "0                                             \n",
       "1                                             \n",
       "2  Le modele n'a jamais prédit cette classe.  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Création du tableau récapitulatif sans la colonne \"Prédictions\"\n",
    "recap_data_corrected = {\n",
    "    \"Classe\": [\n",
    "        f\"{int(class_label)} (Pas de panne)\" if class_label == 0 else\n",
    "        f\"{int(class_label)} (En pleine panne)\" if class_label == 1 else\n",
    "        f\"{int(class_label)} (Panne imminente)\"\n",
    "        for class_label in class_counts_real.index\n",
    "    ],\n",
    "    \"Observations réelles\": class_counts_real.values,\n",
    "    \"Prédictions correctes\": correct_predictions.values,\n",
    "    \"Remarque\": [\n",
    "        \"\" if class_label in class_counts_pred.index else \"Le modele n'a jamais prédit cette classe.\"\n",
    "        for class_label in class_counts_real.index\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Création du DataFrame corrigé\n",
    "recap_df_corrected = pd.DataFrame(recap_data_corrected)\n",
    "\n",
    "# Affichage du tableau récapitulatif sans la colonne des prédictions incorrectes\n",
    "recap_df_corrected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd31ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe7aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd57967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0038237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
