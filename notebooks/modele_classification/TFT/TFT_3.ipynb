{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429d19d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "##                  TFT                  ##\n",
    "###########################################\n",
    "## Chargement des données\n",
    "## Conversion des types\n",
    "## Séparation Train / Test\n",
    "## Préparation des TimeSeriesDataSet\n",
    "## Définition des DataLoaders\n",
    "## Initialisation du modèle TFT\n",
    "## Entraînement du modèle\n",
    "## Sauvegarde et chargement du modèle\n",
    "## Prédiction et affichage des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d78edbdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>TP2</th>\n",
       "      <th>TP3</th>\n",
       "      <th>H1</th>\n",
       "      <th>DV_pressure</th>\n",
       "      <th>Reservoirs</th>\n",
       "      <th>Oil_temperature</th>\n",
       "      <th>Motor_current</th>\n",
       "      <th>COMP</th>\n",
       "      <th>DV_eletric</th>\n",
       "      <th>Towers</th>\n",
       "      <th>MPG</th>\n",
       "      <th>LPS</th>\n",
       "      <th>Pressure_switch</th>\n",
       "      <th>Oil_level</th>\n",
       "      <th>Caudal_impulses</th>\n",
       "      <th>panne</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-04-12 11:20:00</td>\n",
       "      <td>9.128</td>\n",
       "      <td>8.650</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>8.648</td>\n",
       "      <td>55.475</td>\n",
       "      <td>6.0175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-04-12 11:20:10</td>\n",
       "      <td>9.354</td>\n",
       "      <td>8.896</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>8.892</td>\n",
       "      <td>56.525</td>\n",
       "      <td>6.0075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp    TP2    TP3     H1  DV_pressure  Reservoirs  \\\n",
       "0  2020-04-12 11:20:00  9.128  8.650 -0.018       -0.022       8.648   \n",
       "1  2020-04-12 11:20:10  9.354  8.896 -0.016       -0.020       8.892   \n",
       "\n",
       "   Oil_temperature  Motor_current  COMP  DV_eletric  Towers  MPG  LPS  \\\n",
       "0           55.475         6.0175   0.0         1.0     1.0  0.0  0.0   \n",
       "1           56.525         6.0075   0.0         1.0     1.0  0.0  0.0   \n",
       "\n",
       "   Pressure_switch  Oil_level  Caudal_impulses  panne  \n",
       "0              1.0        1.0              1.0      0  \n",
       "1              1.0        1.0              1.0      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# Charger le dataset\n",
    "df = pd.read_csv(r\"..\\..\\..\\..\\Datasources\\MetroPT3_new_imputed_final.csv\", delimiter=\",\", decimal=\".\", index_col=0)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "display(df.head(2))\n",
    "\n",
    "# Dataset commence le 2020-04-12 11:20:00 et se termine le 2020-07-17 06:00:00\n",
    "pannes = [\n",
    "    {'id': 'Panne1',  'start': '2020-04-12 11:50:00', 'end': '2020-04-12 23:30:00'},\n",
    "    {'id': 'Panne2',  'start': '2020-04-18 00:00:00', 'end': '2020-04-18 23:59:00'},\n",
    "    {'id': 'Panne3',  'start': '2020-04-19 00:00:00', 'end': '2020-04-19 01:30:00'},\n",
    "    {'id': 'Panne4',  'start': '2020-04-29 03:20:00', 'end': '2020-04-29 04:00:00'},\n",
    "    {'id': 'Panne5',  'start': '2020-04-29 22:00:00', 'end': '2020-04-29 22:20:00'},\n",
    "    {'id': 'Panne6',  'start': '2020-05-13 14:00:00', 'end': '2020-05-13 23:59:00'},\n",
    "    {'id': 'Panne7',  'start': '2020-05-18 05:00:00', 'end': '2020-05-18 05:30:00'},\n",
    "    {'id': 'Panne8',  'start': '2020-05-19 10:10:00', 'end': '2020-05-19 11:00:00'},\n",
    "    {'id': 'Panne9',  'start': '2020-05-19 22:10:00', 'end': '2020-05-19 23:59:00'},\n",
    "    {'id': 'Panne10', 'start': '2020-05-20 00:00:00', 'end': '2020-05-20 20:00:00'},\n",
    "    {'id': 'Panne11', 'start': '2020-05-23 09:50:00', 'end': '2020-05-23 10:10:00'},\n",
    "    {'id': 'Panne12', 'start': '2020-05-29 23:30:00', 'end': '2020-05-29 23:59:00'},\n",
    "    {'id': 'Panne13', 'start': '2020-05-30 00:00:00', 'end': '2020-05-30 06:00:00'},\n",
    "    {'id': 'Panne14', 'start': '2020-06-01 15:00:00', 'end': '2020-06-01 15:40:00'},\n",
    "    {'id': 'Panne15', 'start': '2020-06-03 10:00:00', 'end': '2020-06-03 11:00:00'},\n",
    "    {'id': 'Panne16', 'start': '2020-06-05 10:00:00', 'end': '2020-06-05 23:59:00'},\n",
    "    {'id': 'Panne17', 'start': '2020-06-06 00:00:00', 'end': '2020-06-06 23:59:00'},\n",
    "    {'id': 'Panne18', 'start': '2020-06-07 00:00:00', 'end': '2020-06-07 14:30:00'},\n",
    "    {'id': 'Panne19', 'start': '2020-07-08 17:30:00', 'end': '2020-07-08 19:00:00'},\n",
    "    {'id': 'Panne20', 'start': '2020-07-15 14:30:00', 'end': '2020-07-15 19:00:00'},\n",
    "    {'id': 'Panne21', 'start': '2020-07-17 04:30:00', 'end': '2020-07-17 05:30:00'}\n",
    "         ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8e1714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min time_idx in df_train: 0, Max: 471114\n",
      "Min time_idx in df_test: 0, Max: 356405\n",
      "Conversion des colonnes catégoriques et de panne terminée.\n",
      "group_id ajouté.\n",
      "Encodeurs catégoriques définis.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convertir timestamp en datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Définir la date de séparation entre Train et Test\n",
    "split_date = \"2020-06-05 23:59:10\"  # Séparation temporelle (Fin de la panne 16)\n",
    "\n",
    "# Séparer les données en train/test\n",
    "df_train = df[df[\"timestamp\"] < split_date].copy()\n",
    "df_test = df[df[\"timestamp\"] >= split_date].copy()\n",
    "\n",
    "# Recalculer time_idx en divisant par 10 pour éviter les grands nombres\n",
    "df_train[\"time_idx\"] = ((df_train[\"timestamp\"] - df_train[\"timestamp\"].min()).dt.total_seconds() // 10).astype(int)\n",
    "df_test[\"time_idx\"] = ((df_test[\"timestamp\"] - df_test[\"timestamp\"].min()).dt.total_seconds() // 10).astype(int)\n",
    "\n",
    "print(f\"Min time_idx in df_train: {df_train['time_idx'].min()}, Max: {df_train['time_idx'].max()}\")\n",
    "print(f\"Min time_idx in df_test: {df_test['time_idx'].min()}, Max: {df_test['time_idx'].max()}\")\n",
    "\n",
    "\n",
    "# Définir les colonnes catégoriques et les forcer en str avant de les convertir en category\n",
    "for col in [\"COMP\", \"DV_eletric\", \"Towers\"]:\n",
    "    df_train[col] = df_train[col].astype(int).astype(str).astype(\"category\")\n",
    "    df_test[col] = df_test[col].astype(int).astype(str).astype(\"category\")\n",
    "\n",
    "# Conversion explicite de panne\n",
    "df_train[\"panne\"] = df_train[\"panne\"].astype(int).astype(str).astype(\"category\")\n",
    "df_test[\"panne\"] = df_test[\"panne\"].astype(int).astype(str).astype(\"category\")\n",
    "\n",
    "print(\"Conversion des colonnes catégoriques et de panne terminée.\")\n",
    "\n",
    "# Ajouter un group_id unique pour tout le dataset\n",
    "df_train[\"group_id\"] = df_train[\"COMP\"].astype(str) + \"_\" + (df_train[\"time_idx\"] // 1000).astype(str)\n",
    "df_test[\"group_id\"] = df_test[\"COMP\"].astype(str) + \"_\" + (df_test[\"time_idx\"] // 1000).astype(str)\n",
    "\n",
    "print(\"group_id ajouté.\")\n",
    "\n",
    "# Définir les encodeurs pour les variables catégoriques\n",
    "categorical_encoders = {\n",
    "    \"COMP\": NaNLabelEncoder(),\n",
    "    \"DV_eletric\": NaNLabelEncoder(),\n",
    "    \"Towers\": NaNLabelEncoder(),\n",
    "}\n",
    "\n",
    "print(\"Encodeurs catégoriques définis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18f37c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usermine\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_forecasting\\data\\timeseries.py:1301: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 12 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__group_id': '0_131'}, {'__group_id__group_id': '0_147'}, {'__group_id__group_id': '0_205'}, {'__group_id__group_id': '0_276'}, {'__group_id__group_id': '0_286'}, {'__group_id__group_id': '0_41'}, {'__group_id__group_id': '0_77'}, {'__group_id__group_id': '0_83'}, {'__group_id__group_id': '0_84'}, {'__group_id__group_id': '0_85'}]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset train créé avec 491829 séquences.\n",
      " Dataset train corrigé avec 491829 séquences.\n",
      " Dataset test corrigé avec 357868 séquences.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Définir l'horizon de prévision et la fenêtre d'observation\n",
    "max_encoder_length = 30  # Fenêtre plus longue pour éviter la suppression\n",
    "max_prediction_length = 3  # Une seule prédiction pour éviter les erreurs\n",
    "\n",
    "train_dataset = TimeSeriesDataSet(\n",
    "    df_train,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"panne\",\n",
    "    group_ids=[\"group_id\"],  # Utiliser un ID unique par série\n",
    "    time_varying_known_reals=[\"TP2\", \"H1\", \"DV_pressure\", \"Oil_temperature\", \"Motor_current\"],\n",
    "    time_varying_known_categoricals=[\"DV_eletric\", \"Towers\"],\n",
    "    max_encoder_length=30,  \n",
    "    max_prediction_length=3,  \n",
    "    min_encoder_length=10,  # Augmenter pour éviter des séquences trop courtes\n",
    "    min_prediction_length=3,\n",
    "    categorical_encoders=categorical_encoders,\n",
    "    allow_missing_timesteps=True\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Dataset train créé avec {len(train_dataset)} séquences.\")\n",
    "\n",
    "test_dataset = TimeSeriesDataSet(\n",
    "    df_test,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"panne\",\n",
    "    group_ids=[\"COMP\"],\n",
    "    time_varying_known_reals=[\"TP2\", \"H1\", \"DV_pressure\", \"Oil_temperature\", \"Motor_current\"],\n",
    "    time_varying_known_categoricals=[\"DV_eletric\", \"Towers\"],\n",
    "    max_encoder_length=10,  # Doit être identique à train_dataset\n",
    "    max_prediction_length=3,  # Doit être identique à train_dataset\n",
    "    min_encoder_length=5,\n",
    "    min_prediction_length=2,\n",
    "    categorical_encoders=categorical_encoders,\n",
    "    allow_missing_timesteps=True\n",
    ")\n",
    "\n",
    "\n",
    "# Correction des noms de clés pour correspondre à ceux attendus par le modèle TFT\n",
    "def fix_dataset_structure(dataset):\n",
    "    for i in range(len(dataset)):\n",
    "        if \"encoder_length\" in dataset[i][0]:\n",
    "            dataset[i][0][\"encoder_lengths\"] = dataset[i][0].pop(\"encoder_length\")\n",
    "        if \"decoder_length\" in dataset[i][0]:\n",
    "            dataset[i][0][\"decoder_lengths\"] = dataset[i][0].pop(\"decoder_length\")\n",
    "\n",
    "# Appliquer la correction aux datasets\n",
    "fix_dataset_structure(train_dataset)\n",
    "fix_dataset_structure(test_dataset)\n",
    "\n",
    "print(f\" Dataset train corrigé avec {len(train_dataset)} séquences.\")\n",
    "print(f\" Dataset test corrigé avec {len(test_dataset)} séquences.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55fe29d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usermine\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:209: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "C:\\Users\\usermine\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:209: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle TFT initialisé avec 21007 paramètres.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usermine\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\__init__.py:171: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
      "  super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "\n",
    "# Définition du modèle TFT (Pas besoin de `LightningModule`)\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    train_dataset,\n",
    "    learning_rate=1e-3,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "\n",
    "# Pas besoin d'assertion sur `LightningModule`\n",
    "print(f\"Modèle TFT initialisé avec {tft.size()} paramètres.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dde9548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "# Création du wrapper LightningModule\n",
    "class TFTLightningModule(LightningModule):\n",
    "    def __init__(self, tft):\n",
    "        super().__init__()\n",
    "        self.model = tft\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self.model(x)\n",
    "        loss = self.model.loss(y_pred, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        if batch is None or \"x\" not in batch or \"y\" not in batch:\n",
    "            print(f\"Batch {batch_idx} mal structuré : {batch}\")\n",
    "            return None  # Évite une erreur bloquante\n",
    "\n",
    "        x = batch[\"x\"]\n",
    "        y = batch[\"y\"]\n",
    "\n",
    "        try:\n",
    "            y_pred = self.model(x)\n",
    "            loss = self.model.loss(y_pred, y)\n",
    "        except KeyError as e:\n",
    "            print(f\"Erreur de clé dans `validation_step`: {e}\")\n",
    "            return None\n",
    "\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.model.configure_optimizers()\n",
    "\n",
    "# Encapsulation du modèle TFT dans `LightningModule`\n",
    "tft_lightning = TFTLightningModule(tft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e5774f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usermine\\AppData\\Local\\Temp\\ipykernel_8692\\3057267033.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_labels = torch.stack([torch.tensor(label) for label in labels])\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Échantillon dataset : ['x_cat', 'x_cont', 'encoder_length', 'decoder_length', 'encoder_target', 'encoder_time_idx_start', 'groups', 'target_scale']\n",
      " Erreur : Batch mal structuré ! Type : <class 'tuple'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type                      | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | model | TemporalFusionTransformer | 21.0 K | train\n",
      "------------------------------------------------------------\n",
      "21.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "21.0 K    Total params\n",
      "0.084     Total estimated model params size (MB)\n",
      "326       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56efcad4ebee4e31be59896e62484d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                               | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 mal structuré : ({'x_cat': tensor([[[1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         ...,\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1]],\n",
      "\n",
      "        [[1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         ...,\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1]],\n",
      "\n",
      "        [[1, 1],\n",
      "         [1, 1],\n",
      "         [1, 0],\n",
      "         ...,\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 1],\n",
      "         [1, 1],\n",
      "         [1, 0],\n",
      "         ...,\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1]],\n",
      "\n",
      "        [[1, 1],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         ...,\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 1]],\n",
      "\n",
      "        [[1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         ...,\n",
      "         [1, 1],\n",
      "         [1, 1],\n",
      "         [1, 0]]]), 'x_cont': tensor([[[ 1.0000,  1.8981, -1.9854,  5.4273,  1.6739,  1.3696],\n",
      "         [ 1.0000,  1.9047, -1.9854,  5.4627,  1.6668,  1.3602],\n",
      "         [ 1.0000,  1.9052, -1.9854,  5.4627,  1.6456,  1.3249],\n",
      "         ...,\n",
      "         [ 1.0000,  1.2149, -1.9854,  3.4482,  1.6456,  1.1108],\n",
      "         [ 1.0000,  1.7343, -1.9854,  4.9780,  1.6633,  1.3037],\n",
      "         [ 1.0000,  1.8981, -1.9860,  5.4576,  1.6562,  1.3308]],\n",
      "\n",
      "        [[ 1.0000,  1.9047, -1.9854,  5.4627,  1.6668,  1.3602],\n",
      "         [ 1.0000,  1.9052, -1.9854,  5.4627,  1.6456,  1.3249],\n",
      "         [ 1.0000,  1.9070, -1.9854,  5.4778,  1.6456,  1.2920],\n",
      "         ...,\n",
      "         [ 1.0000,  1.7343, -1.9854,  4.9780,  1.6633,  1.3037],\n",
      "         [ 1.0000,  1.8981, -1.9860,  5.4576,  1.6562,  1.3308],\n",
      "         [ 1.0000,  1.9047, -1.9854,  5.4576,  1.6668,  1.2932]],\n",
      "\n",
      "        [[ 1.0000,  1.9052, -1.9854,  5.4627,  1.6456,  1.3249],\n",
      "         [ 1.0000,  1.9070, -1.9854,  5.4778,  1.6456,  1.2920],\n",
      "         [ 1.0000,  1.1392, -1.9854,  3.2463,  1.6421,  1.0661],\n",
      "         ...,\n",
      "         [ 1.0000,  1.8981, -1.9860,  5.4576,  1.6562,  1.3308],\n",
      "         [ 1.0000,  1.9047, -1.9854,  5.4576,  1.6668,  1.2932],\n",
      "         [ 1.0000,  1.9047, -1.9854,  5.4526,  1.6456,  1.3073]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0000,  1.9064, -1.9860,  5.4677,  1.6173,  1.3496],\n",
      "         [ 1.0000,  1.9088, -1.9854,  5.4778,  1.6315,  1.3767],\n",
      "         [ 1.0000,  1.3734, -1.9854,  3.9632,  1.6173,  1.1555],\n",
      "         ...,\n",
      "         [ 1.0000,  1.9035, -1.9854,  5.4475,  1.6456,  1.3379],\n",
      "         [ 1.0000,  1.9058, -1.9854,  5.4576,  1.6421,  1.3685],\n",
      "         [ 1.0000,  1.9064, -1.9854,  5.4576,  1.6173,  1.3720]],\n",
      "\n",
      "        [[ 1.0000,  1.9088, -1.9854,  5.4778,  1.6315,  1.3767],\n",
      "         [ 1.0000,  1.3734, -1.9854,  3.9632,  1.6173,  1.1555],\n",
      "         [ 1.0000,  1.7396, -1.9854,  4.9881,  1.6633,  1.2355],\n",
      "         ...,\n",
      "         [ 1.0000,  1.9058, -1.9854,  5.4576,  1.6421,  1.3685],\n",
      "         [ 1.0000,  1.9064, -1.9854,  5.4576,  1.6173,  1.3720],\n",
      "         [ 1.0000,  1.9088, -1.9860,  5.4778,  1.6138,  1.3096]],\n",
      "\n",
      "        [[ 1.0000,  1.3734, -1.9854,  3.9632,  1.6173,  1.1555],\n",
      "         [ 1.0000,  1.7396, -1.9854,  4.9881,  1.6633,  1.2355],\n",
      "         [ 1.0000,  1.8899, -1.9854,  5.4021,  1.6527,  1.2908],\n",
      "         ...,\n",
      "         [ 1.0000,  1.9064, -1.9854,  5.4576,  1.6173,  1.3720],\n",
      "         [ 1.0000,  1.9088, -1.9860,  5.4778,  1.6138,  1.3096],\n",
      "         [ 1.0000,  1.4231, -1.9860,  4.0945,  1.6279,  1.1343]]]), 'encoder_length': tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10]), 'decoder_length': tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])}, tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]]))\n",
      "Batch 1 mal structuré : ({'x_cat': tensor([[[1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         ...,\n",
      "         [1, 1],\n",
      "         [1, 0],\n",
      "         [1, 0]],\n",
      "\n",
      "        [[1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         ...,\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0]],\n",
      "\n",
      "        [[1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         ...,\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         ...,\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0]],\n",
      "\n",
      "        [[1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         ...,\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0]],\n",
      "\n",
      "        [[1, 0],\n",
      "         [1, 0],\n",
      "         [1, 1],\n",
      "         ...,\n",
      "         [1, 0],\n",
      "         [1, 0],\n",
      "         [1, 0]]]), 'x_cont': tensor([[[ 1.0000,  1.7396, -1.9854,  4.9881,  1.6633,  1.2355],\n",
      "         [ 1.0000,  1.8899, -1.9854,  5.4021,  1.6527,  1.2908],\n",
      "         [ 1.0000,  1.8940, -1.9854,  5.4273,  1.6279,  1.3437],\n",
      "         ...,\n",
      "         [ 1.0000,  1.9088, -1.9860,  5.4778,  1.6138,  1.3096],\n",
      "         [ 1.0000,  1.4231, -1.9860,  4.0945,  1.6279,  1.1343],\n",
      "         [ 1.0000,  1.7603, -1.9854,  5.0487,  1.6633,  1.2637]],\n",
      "\n",
      "        [[ 1.0000,  1.8899, -1.9854,  5.4021,  1.6527,  1.2908],\n",
      "         [ 1.0000,  1.8940, -1.9854,  5.4273,  1.6279,  1.3437],\n",
      "         [ 1.0000,  1.8934, -1.9854,  5.4324,  1.6102,  1.3649],\n",
      "         ...,\n",
      "         [ 1.0000,  1.4231, -1.9860,  4.0945,  1.6279,  1.1343],\n",
      "         [ 1.0000,  1.7603, -1.9854,  5.0487,  1.6633,  1.2637],\n",
      "         [ 1.0000,  1.8905, -1.9854,  5.4021,  1.6456,  1.3496]],\n",
      "\n",
      "        [[ 1.0000,  1.8940, -1.9854,  5.4273,  1.6279,  1.3437],\n",
      "         [ 1.0000,  1.8934, -1.9854,  5.4324,  1.6102,  1.3649],\n",
      "         [ 1.0000,  1.8946, -1.9854,  5.4324,  1.6138,  1.3332],\n",
      "         ...,\n",
      "         [ 1.0000,  1.7603, -1.9854,  5.0487,  1.6633,  1.2637],\n",
      "         [ 1.0000,  1.8905, -1.9854,  5.4021,  1.6456,  1.3496],\n",
      "         [ 1.0000,  1.8928, -1.9849,  5.4223,  1.6209,  1.3567]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.0000,  1.8887, -1.9854,  5.4072,  1.6598,  1.2955],\n",
      "         [ 1.0000,  1.8887, -1.9854,  5.4273,  1.6456,  1.2896],\n",
      "         [ 1.0000,  1.8899, -1.9854,  5.4223,  1.6173,  1.3579],\n",
      "         ...,\n",
      "         [ 1.0000,  1.5834, -1.9854,  4.5539,  1.6598,  1.1861],\n",
      "         [ 1.0000,  1.8348, -1.9860,  5.2607,  1.6739,  1.2673],\n",
      "         [ 1.0000,  1.8899, -1.9854,  5.4072,  1.6633,  1.3167]],\n",
      "\n",
      "        [[ 1.0000,  1.8887, -1.9854,  5.4273,  1.6456,  1.2896],\n",
      "         [ 1.0000,  1.8899, -1.9854,  5.4223,  1.6173,  1.3579],\n",
      "         [ 1.0000,  1.8910, -1.9854,  5.4324,  1.6173,  1.3637],\n",
      "         ...,\n",
      "         [ 1.0000,  1.8348, -1.9860,  5.2607,  1.6739,  1.2673],\n",
      "         [ 1.0000,  1.8899, -1.9854,  5.4072,  1.6633,  1.3167],\n",
      "         [ 1.0000,  1.8899, -1.9860,  5.4172,  1.6350,  1.3637]],\n",
      "\n",
      "        [[ 1.0000,  1.8899, -1.9854,  5.4223,  1.6173,  1.3579],\n",
      "         [ 1.0000,  1.8910, -1.9854,  5.4324,  1.6173,  1.3637],\n",
      "         [ 1.0000,  1.6278, -1.9854,  4.6700,  1.6421,  1.2484],\n",
      "         ...,\n",
      "         [ 1.0000,  1.8899, -1.9854,  5.4072,  1.6633,  1.3167],\n",
      "         [ 1.0000,  1.8899, -1.9860,  5.4172,  1.6350,  1.3637],\n",
      "         [ 1.0000,  1.8905, -1.9860,  5.4172,  1.6492,  1.3508]]]), 'encoder_length': tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10]), 'decoder_length': tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])}, tensor([[1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1],\n",
      "        [1, 1, 1]]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usermine\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\usermine\\AppData\\Local\\Temp\\ipykernel_8692\\3057267033.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_labels = torch.stack([torch.tensor(label) for label in labels])\n",
      "C:\\Users\\usermine\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee2afc4919b4421b6dadc68a87593f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'encoder_lengths'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 105\u001b[0m\n\u001b[0;32m     95\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     96\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,\n\u001b[0;32m     97\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m     gradient_clip_val\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m    102\u001b[0m )\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Lancement de l'entraînement\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtft_lightning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Entraînement terminé avec succès !\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    571\u001b[0m     ckpt_path,\n\u001b[0;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    574\u001b[0m )\n\u001b[1;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1024\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1026\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 455\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:320\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 320\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    322\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:192\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m         closure()\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:270\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:171\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 171\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    174\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1302\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[0;32m   1272\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1273\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1277\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1300\u001b[0m \n\u001b[0;32m   1301\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1302\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:154\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[1;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:123\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    122\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 223\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    226\u001b[0m     params_with_grad: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:109\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     98\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     99\u001b[0m     optimizer: Steppable,\n\u001b[0;32m    100\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m    101\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    107\u001b[0m \n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:146\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:131\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 131\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:319\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    317\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m--> 319\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mworld_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:323\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 323\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    326\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:391\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m, in \u001b[0;36mTFTLightningModule.training_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m     13\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 14\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mloss(y_pred, y)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss)\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\Amine-Jupiter\\SESSION6\\ProjetSynthese\\Modele1_Classification\\TFT\\tft_env\\Lib\\site-packages\\pytorch_forecasting\\models\\temporal_fusion_transformer\\__init__.py:430\u001b[0m, in \u001b[0;36mTemporalFusionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    427\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03m    input dimensions: n_samples x time x variables\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m     encoder_lengths \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder_lengths\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    431\u001b[0m     decoder_lengths \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_lengths\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    432\u001b[0m     x_cat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_cat\u001b[39m\u001b[38;5;124m\"\u001b[39m], x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_cat\u001b[39m\u001b[38;5;124m\"\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# concatenate in time dimension\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'encoder_lengths'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_lightning.callbacks import EarlyStopping, TQDMProgressBar\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Fonction collate corrigée\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def tft_collate_fn(batch):\n",
    "    \"\"\" Collate function pour le modèle TFT, en s'assurant que les tailles et types sont cohérents. \"\"\"\n",
    "    \n",
    "    batch = [b for b in batch if b is not None]  \n",
    "    if not batch:\n",
    "        return None  # Éviter une erreur si le batch est vide\n",
    "\n",
    "    try:\n",
    "        feature_dicts = [b[0] for b in batch]\n",
    "        labels = [b[1][0] for b in batch]  \n",
    "\n",
    "        # ✅ Correction des noms des clés\n",
    "        required_keys = [\"encoder_length\", \"decoder_length\", \"x_cat\", \"x_cont\"]\n",
    "        missing_keys = [k for k in required_keys if k not in feature_dicts[0]]\n",
    "        if missing_keys:\n",
    "            raise KeyError(f\"Clés manquantes : {missing_keys}\")\n",
    "\n",
    "        # ✅ Trouver la séquence la plus longue\n",
    "        max_seq_len = max(d[\"x_cat\"].shape[0] for d in feature_dicts)\n",
    "\n",
    "        def pad_tensor(tensor, max_len):\n",
    "            \"\"\" Fonction pour padder les tenseurs à la même longueur \"\"\"\n",
    "            pad_size = max_len - tensor.shape[0]\n",
    "            if pad_size > 0:\n",
    "                pad = torch.zeros((pad_size, *tensor.shape[1:]), dtype=tensor.dtype)\n",
    "                return torch.cat([tensor, pad], dim=0)\n",
    "            return tensor\n",
    "\n",
    "        # ✅ Appliquer le padding aux séquences pour uniformiser la taille\n",
    "        batch_data = {\n",
    "            k: torch.stack([pad_tensor(d[k], max_seq_len) for d in feature_dicts])\n",
    "            for k in [\"x_cat\", \"x_cont\"]\n",
    "        }\n",
    "\n",
    "        # ✅ Empiler `encoder_length` et `decoder_length`\n",
    "        batch_data[\"encoder_length\"] = torch.tensor([d[\"encoder_length\"] for d in feature_dicts])\n",
    "        batch_data[\"decoder_length\"] = torch.tensor([d[\"decoder_length\"] for d in feature_dicts])\n",
    "\n",
    "        # ✅ Conversion des labels en tenseur\n",
    "        batch_labels = torch.stack([torch.tensor(label) for label in labels])\n",
    "\n",
    "        return batch_data, batch_labels\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur `tft_collate_fn`: {e}\")\n",
    "        return None  \n",
    "\n",
    "\n",
    "# Vérification du dataset\n",
    "try:\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"Échantillon dataset : {list(sample[0].keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors du chargement du dataset : {e}\")\n",
    "\n",
    "# Configuration des DataLoaders\n",
    "batch_size = 64\n",
    "num_workers = 0  #Éviter multiprocessing sous Windows\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=False, \n",
    "    collate_fn=tft_collate_fn, pin_memory=pin_memory\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False, \n",
    "    collate_fn=tft_collate_fn, pin_memory=pin_memory\n",
    ")\n",
    "\n",
    "# Vérification du premier batch\n",
    "try:\n",
    "    test_batch = next(iter(train_dataloader))\n",
    "    if isinstance(test_batch, tuple) or test_batch is None:\n",
    "        print(f\" Erreur : Batch mal structuré ! Type : {type(test_batch)}\")\n",
    "    else:\n",
    "        print(f\" Batch structuré : Clés = {list(test_batch.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\" Erreur lors du chargement du batch : {e}\")\n",
    "\n",
    "# Callbacks pour l'entraînement\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=5, verbose=True, mode=\"min\")\n",
    "progress_bar = TQDMProgressBar(refresh_rate=10)\n",
    "\n",
    "# Définition de l'entraîneur\n",
    "trainer = Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator=\"auto\",\n",
    "    enable_progress_bar=True,\n",
    "    enable_checkpointing=False,  \n",
    "    callbacks=[early_stop_callback, progress_bar],  \n",
    "    gradient_clip_val=0.1\n",
    ")\n",
    "\n",
    "# Lancement de l'entraînement\n",
    "trainer.fit(tft_lightning, train_dataloader, test_dataloader)\n",
    "\n",
    "print(\" Entraînement terminé avec succès !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0aab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "## 🔟 Sauvegarde et chargement du modèle    ##\n",
    "##############################################\n",
    "\n",
    "model_path = r\"..\\..\\Generated_Files\\TFT\\tft_model.ckpt\"\n",
    "trainer.save_checkpoint(model_path)\n",
    "print(\"Modèle sauvegardé.\")\n",
    "\n",
    "tft_loaded = TemporalFusionTransformer.load_from_checkpoint(model_path)\n",
    "print(\"Modèle chargé avec succès.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a55bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "## 1️⃣1️⃣ Prédictions avec le modèle       ##\n",
    "##############################################\n",
    "\n",
    "predictions = tft.predict(test_dataloader, mode=\"prediction\")\n",
    "\n",
    "print(\"Prédictions :\", predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca3a0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45f90ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TFT)",
   "language": "python",
   "name": "tft_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
